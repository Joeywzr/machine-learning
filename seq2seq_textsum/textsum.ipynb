{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 概述\n",
    "神经网络文本摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf  # 1.0.1\n",
    "from tensorflow.python.ops import variable_scope\n",
    "\n",
    "%matplotlib inline\n",
    "# %load_ext watermark\n",
    "# %watermark -p tensorflow,numpy -v -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 模型相关参数\n",
    "cell_size = 100\n",
    "num_layers = 2  # rnn cell 层数\n",
    "embedding_size = 100\n",
    "buckets = [(30, 14), (40, 14), (50, 14)]\n",
    "num_sampled = 1000\n",
    "\n",
    "# 语料读取参数\n",
    "lines_to_read_train = 110000  # 取0表示读取整个文件\n",
    "lines_to_read_dev = 9000  # 取0表示读取整个文件\n",
    "vocab_min_freq_enc = 3  # 计入词表的最小词频, 小于此值则记为 UNK\n",
    "vocab_min_freq_dec = 2  # 计入词表的最小词频, 小于此值则记为 UNK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 读入语料并处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "basedir = '^data/'\n",
    "files = {'train': {'from': basedir + 'train_contents.txt',\n",
    "                   'to': basedir + 'train_titles.txt'},\n",
    "         'dev': {'from': basedir + 'dev_contents.txt',\n",
    "                  'to': basedir + 'dev_titles.txt'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 文本预处理\n",
    "\n",
    "见 `data.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 读取文本文件, tokenize 得到 sentences (word lists)\n",
    "\n",
    "decoder inputs 在句子首尾分别添加特殊 token\n",
    "* 句首的 <GO> Token 是为了喂给 decoder 的第一个 cell, 以便生成第一个词.  \n",
    "* 句末的 <EOS> Token 相当于人为给模型提供一个句子结束的信号, 让模型自己学会何时结束句子."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_PAD, _GO, _EOS, _UNK = '<PAD>', '<GO>', '<EOS>', '<UNK>' \n",
    "_PAD_ID, _GO_ID, _EOS_ID, _UNK_ID = 0, 1, 2, 3\n",
    "_START_VOCAB = (_PAD, _GO, _EOS, _UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.835 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "def en_tokenizer(sentence):\n",
    "    \"\"\"英文句子分词\n",
    "    source: https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/data_utils.py\n",
    "    \"\"\"\n",
    "    _word_split = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(_word_split.split(space_separated_fragment))\n",
    "    return [w for w in words if w]\n",
    "\n",
    "def zh_tokenizer(sentence):\n",
    "    '''中文句子分词'''\n",
    "    return [w for w in jieba.cut(sentence.strip()) \n",
    "            if w not in (' ', '\\t', '\\n',)]\n",
    "    # return [char for char in sentence.strip() if char not in (' ', '\\t', '\\n')]\n",
    "\n",
    "\n",
    "def read_sentences(encoder_file, decoder_file, lines_to_read=None):\n",
    "    \"\"\"默认从英文到中文\"\"\"\n",
    "    \n",
    "    # 读取文件. lines_to_read 参数可用于小规模试验.\n",
    "    with open(encoder_file, encoding='utf-8') as f:\n",
    "        enc_lines = f.readlines()[:lines_to_read] if lines_to_read else f.readlines()\n",
    "    with open(decoder_file, encoding='utf-8') as f:\n",
    "        dec_lines = f.readlines()[:lines_to_read] if lines_to_read else f.readlines()\n",
    "    assert len(enc_lines) == len(dec_lines)\n",
    "\n",
    "    num_lines = len(enc_lines)\n",
    "    \n",
    "    encoder_sentences = []\n",
    "    decoder_sentences = []\n",
    "    \n",
    "    for i in range(num_lines):\n",
    "        enc_sentence = zh_tokenizer(enc_lines[i])\n",
    "        dec_sentence = zh_tokenizer(dec_lines[i])\n",
    "        if enc_sentence and dec_sentence:\n",
    "            encoder_sentences.append(enc_sentence)\n",
    "            decoder_sentences.append(dec_sentence)\n",
    "    return encoder_sentences, decoder_sentences\n",
    "\n",
    "\n",
    "encoder_sentences, decoder_sentences = read_sentences(\n",
    "    files['train']['from'], files['train']['to'], lines_to_read=lines_to_read_train)\n",
    "encoder_sentences_test, decoder_sentences_test = read_sentences(\n",
    "    files['dev']['from'], files['dev']['to'], lines_to_read=lines_to_read_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6cb4b53e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_length_histogram(sentences):\n",
    "    lengths = np.asarray([len(s) for s in sentences])\n",
    "    max_len = np.max(lengths)\n",
    "    plt.hist(lengths, bins=range(0, max_len, int(max_len/20) or 1));\n",
    "\n",
    "plot_length_histogram(encoder_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wG4d9bJIGCNNgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6cb4b53a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_length_histogram(decoder_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 由训练文件的 word lists 得到 vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80030 44907\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(sentences, vocab_min_freq):\n",
    "    \"\"\"生成词表. 前几个位置留给 _START_VOCAB 的特殊 token \"\"\"\n",
    "    vocab = list(_START_VOCAB)\n",
    "    words_flat = [w for s in sentences for w in s]\n",
    "    word_cnt = Counter(words_flat)\n",
    "    for word, count in word_cnt.most_common():\n",
    "        if count >= vocab_min_freq:\n",
    "            vocab.append(word)\n",
    "    return vocab\n",
    "\n",
    "# vocab_xxx 相当于 demo 代码里的 reversed_vocab\n",
    "vocab_enc = build_vocab(encoder_sentences, vocab_min_freq_enc)  # list: id -> word\n",
    "vocab_dec = build_vocab(decoder_sentences, vocab_min_freq_dec)\n",
    "num_encoder_symbols = len(vocab_enc)\n",
    "num_decoder_symbols = len(vocab_dec)\n",
    "\n",
    "# wod2id_xxx 相当于 demo 代码里的 vocab\n",
    "word2id_enc = {w: i for i, w in enumerate(vocab_enc)}  # dict: word -> id\n",
    "word2id_dec = {w: i for i, w in enumerate(vocab_dec)}\n",
    "\n",
    "print(num_encoder_symbols, num_decoder_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### decoder inputs 句子首尾加 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def add_tokens(sentences):\n",
    "    \"\"\"为 decoder 的输入语句增加首尾 token\"\"\"\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = [_GO] + sentences[i] + [_EOS]\n",
    "    return sentences\n",
    "\n",
    "decoder_sentences = add_tokens(decoder_sentences)\n",
    "decoder_sentences_test = add_tokens(decoder_sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### tokens -> ids -> bucketing -> padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([12891, 13524, 15202], [1077, 1100, 1240])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bucket_and_pad(enc_sentences, dec_sentences, buckets, word2id_enc, word2id_dec):\n",
    "    \"\"\"\n",
    "    - enc_sentences: A nested list of symbol str for encoding, length: batch_size\n",
    "    - dec_sentences: A nested list of symbol str for decoding, length: batch_size\n",
    "    - word2id_enc, word2id_dec: dict. symbol (str) -> index (int)\n",
    "    \n",
    "    Example: \n",
    "    [\"hello\", \"world\"] -> [\"hi\", \"<EOS>\"]\n",
    "    [\"cover\", \"me\"] -> [\"roger\", \"<EOS>\"]\n",
    "        \n",
    "    Assume that index of \"<PAD>\" is 0\n",
    "\n",
    "    Output:\n",
    "    [[0, 0, <index of 'hello'>, <index of 'world'>], [0, 0, <index of 'cover'>, <index of 'me'>]],\n",
    "    [[<index of 'hi'>, <index of 'EOS'>, 0, 0], [<index of 'roger'>, <index of 'EOS'>, 0, 0]]\n",
    "    \"\"\"\n",
    "    def to_index(sentence, length, word2id, pad_from_start=True):\n",
    "        ids = [_PAD_ID] * length\n",
    "        l = len(sentence)\n",
    "        if l < length:\n",
    "            if pad_from_start:\n",
    "                ids[(length - l):] = [word2id.get(w, _UNK_ID) for w in sentence]\n",
    "            else:\n",
    "                ids[:l] = [word2id.get(w, _UNK_ID) for w in sentence]\n",
    "        else:\n",
    "            ids = [word2id.get(w, _UNK_ID) for w in sentence[:length]]\n",
    "        return ids\n",
    "    \n",
    "    num_sentences = len(enc_sentences)\n",
    "    \n",
    "    encoder_data = [[] for _ in range(len(buckets))]\n",
    "    decoder_data = [[] for _ in range(len(buckets))]\n",
    "    \n",
    "    # bucketing. 此时 decoder_sentences 已加首尾 token.\n",
    "    for i in range(num_sentences):\n",
    "        for bucket_id, (encoder_size, decoder_size) in enumerate(buckets):\n",
    "            if len(enc_sentences[i]) <= encoder_size and len(dec_sentences[i]) <= decoder_size:\n",
    "                encoder_data[bucket_id].append(\n",
    "                    to_index(enc_sentences[i], encoder_size, word2id_enc, True))\n",
    "                decoder_data[bucket_id].append(\n",
    "                    to_index(dec_sentences[i], decoder_size, word2id_dec, False))\n",
    "                break\n",
    "    \n",
    "    return encoder_data, decoder_data\n",
    "\n",
    "\n",
    "encoder_data, decoder_data = bucket_and_pad(\n",
    "    encoder_sentences, decoder_sentences, buckets, word2id_enc, word2id_dec)\n",
    "encoder_data_test, decoder_data_test = bucket_and_pad(\n",
    "    encoder_sentences_test, decoder_sentences_test, buckets, word2id_enc, word2id_dec)\n",
    "\n",
    "data_sizes = [len(encoder_data[i]) for i in range(len(buckets))]\n",
    "data_sizes_dec = [len(decoder_data[i]) for i in range(len(buckets))]\n",
    "assert data_sizes == data_sizes_dec\n",
    "\n",
    "data_sizes_test = [len(encoder_data_test[i]) for i in range(len(buckets))]\n",
    "\n",
    "data_sizes, data_sizes_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 构建 seq2seq 模型\n",
    "\n",
    "\n",
    "训练和生成(decoding) 这两个阶段需要共享变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def single_cell():\n",
    "    return tf.contrib.rnn.GRUCell(cell_size)\n",
    "cell = single_cell()\n",
    "if num_layers > 1:\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 按最大 bucket 的长度创建 placeholder 列表\n",
    "\n",
    "注意在 bucketing 时, decoder inputs 已经做了首尾的 token: GO 和 EOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_encoder_length, max_decoder_length = buckets[-1]\n",
    "\n",
    "encoder_placeholders = [\n",
    "    tf.placeholder(tf.int32, shape=[None], name=\"encoder_%d\" % i)\n",
    "    for i in range(max_encoder_length)]\n",
    "decoder_placeholders = [\n",
    "    tf.placeholder(tf.int32, shape=[None], name=\"decoder_%d\" % i)\n",
    "    for i in range(max_decoder_length)]\n",
    "target_placeholders = [\n",
    "    tf.placeholder(tf.int32, shape=[None], name=\"target_%d\" % i)\n",
    "    for i in range(max_decoder_length)]\n",
    "target_weights_placeholders = [\n",
    "    tf.placeholder(tf.float32, shape=[None], name=\"decoder_weight_%d\" % i)\n",
    "    for i in range(max_decoder_length)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 利用 `model_with_buckets` 接口构建模型\n",
    "\n",
    "注: tf 1.0.1 ok. tf 1.1.0 接口有变, 会报错."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_f(encoder_placeholders, decoder_placeholders, do_decode):\n",
    "    # 可以在这里设置不同的 seq2seq 接口, 比如 embedding_attention_seq2seq\n",
    "    return tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(\n",
    "        encoder_placeholders,\n",
    "        decoder_placeholders,\n",
    "        cell,\n",
    "        num_encoder_symbols=num_encoder_symbols,\n",
    "        num_decoder_symbols=num_decoder_symbols,\n",
    "        embedding_size=embedding_size,\n",
    "        output_projection=output_projection,\n",
    "        feed_previous=do_decode)\n",
    "\n",
    "def sampled_loss(labels, logits):\n",
    "    \"\"\"参考: https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py\"\"\"\n",
    "    labels = tf.reshape(labels, [-1, 1])\n",
    "    # We need to compute the sampled_softmax_loss using 32bit floats to\n",
    "    # avoid numerical instabilities.\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "        weights=softmax_w_t,\n",
    "        biases=softmax_b,\n",
    "        labels=labels,\n",
    "        inputs=logits,\n",
    "        num_sampled=num_sampled,\n",
    "        num_classes=num_decoder_symbols)\n",
    "\n",
    "softmax_w_t = tf.get_variable(\"proj_w\", [num_decoder_symbols, cell_size], dtype=tf.float32)\n",
    "softmax_w = tf.transpose(softmax_w_t)\n",
    "softmax_b = tf.get_variable(\"proj_b\", [num_decoder_symbols], dtype=tf.float32)\n",
    "output_projection = (softmax_w, softmax_b)\n",
    "\n",
    "outputs, losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n",
    "    encoder_placeholders, decoder_placeholders, target_placeholders,\n",
    "    target_weights_placeholders, buckets, lambda x, y: seq2seq_f(x, y, False),\n",
    "    softmax_loss_function=sampled_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "losses 是个列表, 各元素分别为不同 bucket 的 loss. 用 losses 求和后传给 train_step 会报错, 原因可能是: 较大 bucket 的 losses 需要 feed 更多的 placeholder, 在用较小 bucket 的数据来训练时, 无法提供.\n",
    "\n",
    "实际上不同 bucket 的训练是相对独立的, 因此可以用不同的 loss 传给 optimizer 用于不同 bucket 的训练. 参见 [demon386/tf_bucket_seq2seq/bucketmodel.py](https://github.com/demon386/tf_bucket_seq2seq/blob/master/bucketmodel.py) 之 [L173](https://github.com/demon386/tf_bucket_seq2seq/blob/master/bucketmodel.py#L173), [L243](https://github.com/demon386/tf_bucket_seq2seq/blob/master/bucketmodel.py#L243)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 准备 feed 数据\n",
    "\n",
    "注意 embedding_rnn_seq2seq 接收的 encoder_inputs 形状为 `A list of 1D int32 Tensors of shape [batch_size]`. 为此, 可以用 `list(zip(*lst))` 来对 nested list 进行\"转置\", 得到需要的形状."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def left_shift(decoder_inputs):\n",
    "    \"\"\"generate targets grom decoder_inputs\"\"\"\n",
    "    return [list(input_[1:]) + [_PAD_ID] for input_ in decoder_inputs]\n",
    "\n",
    "def get_bucket_inputs(encoder_data, decoder_data, bucket_id):\n",
    "    encoder_inputs = encoder_data[bucket_id]\n",
    "    decoder_inputs = decoder_data[bucket_id]\n",
    "    return (encoder_inputs, decoder_inputs)\n",
    "\n",
    "def get_batch_inputs(encoder_data, decoder_data, bucket_id, batch_start, batch_size):\n",
    "    encoder_inputs = encoder_data[bucket_id][batch_start : batch_start+batch_size]\n",
    "    decoder_inputs = decoder_data[bucket_id][batch_start : batch_start+batch_size]\n",
    "    return (encoder_inputs, decoder_inputs)\n",
    "\n",
    "def generate_feed_dict(inputs_tuple, encoder_size, decoder_size):\n",
    "    \"\"\"对 inputs 做转置, 并喂给 placeholder 列表, 得到 feed_dict\"\"\"\n",
    "    encoder_inputs, decoder_inputs = inputs_tuple\n",
    "    encoder_inputs = list(zip(*encoder_inputs))\n",
    "    target_inputs = list(zip(*left_shift(decoder_inputs)))\n",
    "    decoder_inputs = list(zip(*decoder_inputs)) \n",
    "    \n",
    "    feed_dict = dict()\n",
    "    # Prepare input data\n",
    "    for i in range(encoder_size):\n",
    "        # 这里用 placeholder 或者 placeholder.name 都可以\n",
    "        feed_dict[encoder_placeholders[i].name] = np.asarray(encoder_inputs[i], dtype=int)\n",
    "    for i in range(decoder_size):\n",
    "        feed_dict[decoder_placeholders[i].name] = np.asarray(decoder_inputs[i], dtype=int)\n",
    "        feed_dict[target_placeholders[i].name] = np.asarray(target_inputs[i], dtype=int)        \n",
    "        # 这里使用 weights 把 <PAD> 的损失屏蔽了\n",
    "        feed_dict[target_weights_placeholders[i].name] = np.asarray(\n",
    "            [float(idx != _PAD_ID) for idx in target_inputs[i]], dtype=float)\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time           Epoch bucket-0 bucket-1 bucket-2\n",
      "\n",
      "05-12 14:00:07    0   8.8301   6.4359   6.3408 \n",
      "05-12 14:07:28    5   5.6112   4.9979   5.0296 \n",
      "05-12 14:14:41   10   4.1530   3.9522   4.0271 \n",
      "05-12 14:21:52   15   3.4243   3.3170   3.3448 \n",
      "05-12 14:29:05   20   2.9079   2.8229   2.8279 \n",
      "05-12 14:36:17   25   2.4727   2.3874   2.4014 \n",
      "05-12 14:43:28   30   2.0895   2.0396   2.0371 \n",
      "05-12 14:50:41   35   1.8414   1.7646   1.7627 \n",
      "05-12 14:57:56   40   1.5043   1.4883   1.4724 \n",
      "05-12 15:05:05   45   1.3163   1.2867   1.2993 \n",
      "05-12 15:12:20   50   1.2047   1.1736   1.1979 \n",
      "05-12 15:19:33   55   1.0155   0.9947   1.0065 \n",
      "05-12 15:26:47   60   0.9226   0.9203   0.9106 \n",
      "05-12 15:33:57   65   0.8202   0.7940   0.8168 \n",
      "05-12 15:41:12   70   0.7093   0.7014   0.7146 \n",
      "05-12 15:48:24   75   0.6348   0.6358   0.6518 \n",
      "05-12 15:55:33   80   0.5959   0.5800   0.6129 \n",
      "05-12 16:02:43   85   0.5509   0.5219   0.5364 \n",
      "05-12 16:09:53   90   0.4787   0.4713   0.4936 \n",
      "05-12 16:17:06   95   0.4300   0.4124   0.4333 \n",
      "05-12 16:24:20  100   0.4495   0.4269   0.4495 \n",
      "05-12 16:31:32  105   0.4901   0.4873   0.5172 \n",
      "05-12 16:38:43  110   0.8497   0.8661   0.8867 \n",
      "05-12 16:45:56  115   0.8428   0.7941   0.8264 \n",
      "05-12 16:53:09  120   0.6232   0.5803   0.5791 \n",
      "05-12 17:00:20  125   0.4016   0.3978   0.4202 \n",
      "05-12 17:07:33  130   0.2711   0.2841   0.2878 \n",
      "05-12 17:14:44  135   0.1991   0.2137   0.2264 \n",
      "05-12 17:21:56  140   0.1645   0.1790   0.1816 \n",
      "05-12 17:29:06  145   0.1330   0.1508   0.1609 \n",
      "05-12 17:36:15  150   0.1175   0.1374   0.1426 \n",
      "05-12 17:43:29  155   0.1047   0.1233   0.1282 \n",
      "05-12 17:50:36  160   0.0947   0.1118   0.1147 \n",
      "05-12 17:57:40  165   0.0856   0.1063 \n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# 训练相关参数\n",
    "epochs = 500\n",
    "print_loss_every = 5\n",
    "learning_rate = 1\n",
    "batch_size = 50\n",
    "\n",
    "# 把不同 bucket 的 loss 分别传给 optimizer, 得到不同的 train_step.\n",
    "train_steps = [tf.train.AdagradOptimizer(learning_rate).minimize(losses[i]) \n",
    "               for i in range(len(buckets))]\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Training\n",
    "try:\n",
    "    print('{:14}'.format('Time') +\n",
    "          ' Epoch ' +\n",
    "          ' '.join(['bucket-{}'.format(i) for i in range(len(buckets))]))\n",
    "    for i in range(epochs):\n",
    "        if i % print_loss_every == 0:\n",
    "            print('\\n{} {: 4d}'.format(str(datetime.now())[5:-7], \n",
    "                                       i), end=' ')\n",
    "\n",
    "        for bucket_id in range(len(buckets)):\n",
    "            cur_data_size = data_sizes[bucket_id]\n",
    "            if cur_data_size == 0:\n",
    "                continue  # 某个 bucket 为空的特殊情形\n",
    "            encoder_size, decoder_size = buckets[bucket_id]\n",
    "            \n",
    "            # 输出 loss 过程信息\n",
    "            if i % print_loss_every == 0:\n",
    "                bucket_inputs = get_bucket_inputs(encoder_data, decoder_data, bucket_id)\n",
    "                bucket_feed = generate_feed_dict(bucket_inputs, encoder_size, decoder_size)\n",
    "                loss_val = sess.run(losses[bucket_id], feed_dict=bucket_feed)\n",
    "                print('{: 8.4f}'.format(loss_val), end=' ')\n",
    "            \n",
    "            # 训练\n",
    "            for batch_start in range(0, cur_data_size, batch_size):\n",
    "                batch_inputs = get_batch_inputs(\n",
    "                    encoder_data, decoder_data, bucket_id, batch_start, batch_size)\n",
    "                batch_feed = generate_feed_dict(batch_inputs, encoder_size, decoder_size)\n",
    "                sess.run(train_steps[bucket_id], feed_dict=batch_feed)\n",
    "except KeyboardInterrupt:\n",
    "    print('\\nKeyboardInterrupt')\n",
    "\n",
    "# plt.plot(range(0, i, print_loss_every), loss_history);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 测试/decode\n",
    "\n",
    "decoding 过程中, 因为 feed_previous 为 true, 所以 `decoder_inputs` 只用到第一个元素, 后面的都不需要."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** bucket 0\n",
      "test examples: 1077\n",
      "loss =  10.0769\n",
      " input:  俄罗斯 <UNK> 公司 已经 宣布 公开招标 建造 世界 最大 的 核动力 破冰船 。 公司 要求 2017 年底 该船 能够 交付使用 。\n",
      "output:  <UNK> <UNK> <UNK> <UNK> 男子 为 <UNK> 应用 功能 称 没有 者 都 可\n",
      "target:  俄罗斯 计划 建造 世界 最大 的 核动力 破冰船 <EOS>\n",
      "\n",
      " input:  株洲 新闻网 7 月 27 日讯 今天上午 , 光大银行 株洲 分行 与 <UNK> 签署 战略 合作 协议 , 光大银行 <UNK> 金融 专业 支行 也 正式 挂牌 。\n",
      "output:  <UNK> 集团 职业 技术 学院 与 金融 开业 <EOS>\n",
      "target:  光大银行 <UNK> 金融 支行 成立 服务 <UNK> 商圈 / 图 <EOS>\n",
      "\n",
      " input:  中新网 北京 6 月 27 日电 中国财政部 部长 谢旭人 27 日 在 北京 表示 , 2011 年 中国 国防 支出 5829.62 亿元 。\n",
      "output:  中国 去年 国防 支出 5829 亿元 公共安全 支出 1037 亿 <EOS>\n",
      "target:  2011 年 中国 国防 支出 5829.62 亿元 <EOS>\n",
      "\n",
      " input:  6 月 25 日 早盘 , 亚太 主要 股票市场 全线 下跌 , 日 股盘 初 跌 0.48 % 。\n",
      "output:  亚太 主要 股市 早盘 全线 上涨 日 30 跌 1.6 % <EOS>\n",
      "target:  亚太 主要 股市 早盘 全线 下跌 日股 跌 0.48 % <EOS>\n",
      "\n",
      " input:  丁力 于鸿君 马燕军 王力军 王 英杰 王 荣军 王贵平 王晓霞 王海平 王敬东 王 智慧 邓志荣 甘荣坤 叶青纯 边学愚 刚杰 仲兆军 刘志 刘云广 刘 江平 刘春锋 刘振刚 芦育珠 李军\n",
      "output:  市长 会见 纪律检查 委员会 管理 名单 <EOS>\n",
      "target:  北京市委 第十一届 纪律检查 委员会 委员 名单 <EOS>\n",
      "\n",
      " input:  全国 科技 创新 大会 7 月 6 日至 7 日 在 北京 举行 。\n",
      "output:  <UNK> <UNK> 全国 中学生 录取 全国 海洋 研究 事件 <EOS>\n",
      "target:  我国 提出 到 2020 年 进入 创新型 国家 行列 目标 <EOS>\n",
      "\n",
      " input:  本文 摘自 : 人民网 , 作者 : 佚名 , 原题 : 《 1854 年 7 月 5 日 公布 》\n",
      "output:  《 <UNK> <UNK> 》 拍 <UNK> 拍 <EOS>\n",
      "target:  《 上海 <UNK> <UNK> <UNK> 》 公布 <EOS>\n",
      "\n",
      " input:  太平洋 证券 日前 发布 有色金属 行业 研报 。 研报 指出 , 上 半个 月 , 股票 与 期货市场 在 <UNK> 传闻 与 经济 下滑 正 负面 信息 中 <UNK> 。\n",
      "output:  深圳 银行 研究 创新 大 扶持 扶持 资金 <EOS>\n",
      "target:  太平洋 证券 : 关注 黄金 、 基本 金属 类股票 <EOS>\n",
      "\n",
      " input:  本报讯 省 水利 投资 集团 有限公司 与 寿宁县 政府 合作 的 寿宁县 城 水务 项目 26 日 启动 , 省 水利 投资 集团 水务 有限公司 同时 授牌 。\n",
      "output:  山东省 新区 设 太阳能 <UNK> 技术 <EOS>\n",
      "target:  <UNK> 城 水务 项目 启动 <EOS>\n",
      "\n",
      " input:  6 月 26 日 , 记者 从省 统计局 获悉 , 今年以来 , 我省 产业 集聚 区 制造业 投资 稳定 快速增长 , 推动 投资 结构 进一步 优化 。\n",
      "output:  新疆 <UNK> 农信社 为 市 调研 <EOS>\n",
      "target:  河南省 产业 集聚 区 投资 结构 进一步 优化 <EOS>\n",
      "\n",
      " input:  据 朝中社 报道 , 朝鲜劳动党 中央军事委员会 和 朝鲜 国防委员会 16 日 发布 决定 , 授予 玄永哲 朝鲜人民军 次帅 军衔 。\n",
      "output:  <UNK> 大楼 至 “ <UNK> ” <EOS>\n",
      "target:  玄永哲 曾 与 金正恩 一同 升 大将 个人信息 尚未 公开 <EOS>\n",
      "\n",
      " input:  胜利 大街 12 甲 1 门 门前 新兴 街 2 - 1 号 皇姑区 黄河 南大街 13 号 楼下\n",
      "output:  大巴 可 建成 10 年 儿子 救助 <UNK> 儿子 受伤 <EOS>\n",
      "target:  街边 IC卡 <UNK> 破损 严重 <EOS>\n",
      "\n",
      " input:  本报 济南 讯 6 月 25 日 , 省 文物局 和 沂水县 人民政府 《 合作 加强 沂水县 <UNK> 崮 古墓 保护 工作 协议 签字仪式 》 在 济南 举行 。\n",
      "output:  《 青海省 区 经济 湿地 工程 进行 出版 <EOS>\n",
      "target:  <UNK> 协议 加强 沂水 <UNK> 崮 古墓 保护 <EOS>\n",
      "\n",
      " input:  6 月 25 日 早盘 , 亚太 主要 股票市场 全线 下跌 , 日 股盘 初 跌 0.48 % 。\n",
      "output:  亚太 主要 股市 早盘 全线 上涨 日 30 跌 1.6 % <EOS>\n",
      "target:  亚太 主要 股市 早盘 全线 下跌 日股 跌 0.48 % <EOS>\n",
      "\n",
      " input:  6 月 18 日 , 英国 大使馆 文化教育 处在 中国 正式 发布 网络 学习 平台 <UNK> <UNK> 。\n",
      "output:  英国 使馆 <UNK> 发布 英语 学习 平台 <EOS>\n",
      "target:  英 文化 协会 发布 英语 学习 平台 <EOS>\n",
      "\n",
      " input:  市委 委员 、 普陀 区委书记 <UNK>\n",
      "output:  昆明 设 企业 建立 金融 新区 工程 选拔 工作 <EOS>\n",
      "target:  张国洪 : 普陀 重点 寻求 第三产业 突破 适应 中心 城区 发展 <EOS>\n",
      "\n",
      " input:  图为 江西 石城县 举行 的 赣南 客家 婚俗 “ 打 甑 盖 ” 表演 , 一位 女子 被 众人 用 <UNK> 敲打 头顶 上 的 甑 盖 。 刘占昆 摄\n",
      "output:  “ <UNK> 举行 15 年 2012 年 甘肃 年 游客 <EOS>\n",
      "target:  江西 <UNK> 再现 赣南 客家 婚俗 “ 打 <UNK> 盖 ” <EOS>\n",
      "\n",
      " input:  7 月 25 日 凌晨 5 时至 26 日 7 时 , 天津 普降 大到暴雨 。 图为 马场 道 与 友谊路 交口 附近 。\n",
      "output:  天津 公安局 : 26 日 因雨 坏车 被 摄录 一律 不 处罚 <EOS>\n",
      "target:  天津市 公安局 : 26 日 因雨 坏车 被 摄录 一律 不 处罚 <EOS>\n",
      "\n",
      " input:  昨天 , 诺基亚公司 公布 的 二 季报 显示 , 其 季度 亏损 已经 高达 14 亿 欧元 , 同比 扩大 了 近 3 倍 。\n",
      "output:  苹果 股票 再 获 数据 称 竣工 / / / / 美元 得 亮相\n",
      "target:  诺基亚 二季度 亏损 17 亿美元 同比 扩大 近 3 倍 <EOS>\n",
      "\n",
      " input:  共 6 种 : <UNK> 、 <UNK> 、 老年 优惠卡 、 老年 免费 卡 、 学生 卡 、 <UNK>\n",
      "output:  2011 年 20 年 援助 “ <UNK> ” 达 计划 <EOS>\n",
      "target:  昆明 <UNK> 卡 亮相 <EOS>\n",
      "\n",
      "\n",
      "** bucket 1\n",
      "test examples: 1100\n",
      "loss =   9.1063\n",
      " input:  昨天 , 大连 阿尔滨 足球队 训练 基地 暨 足球 公园 举行 开工典礼 。 这个 项目 的 开工 建设 , 标志 着 大连 阿尔滨 足球队 即将 结束 没有 自己 的 训练 基地 的 历史 。\n",
      "output:  甘肃省 农大 交流 新疆 选举 <EOS>\n",
      "target:  阿尔滨 训练 基地 <UNK> 占地 31 万平方米 拥有 11 块 草坪 <EOS>\n",
      "\n",
      " input:  三峡 晚报 讯 15 日 上午 8 时 左右 , <UNK> 嘴 公园 一带 在 施工 时 , 施工 队伍 不慎 挖断 了 自来水管 道 , 造成 300 多 住户 用水 受 影响 。\n",
      "output:  <UNK> 铁路局 三天 归还 体验 <EOS>\n",
      "target:  施工 <UNK> 水管 , 流 了 百余 吨 <EOS>\n",
      "\n",
      " input:  6 月 17 日 父亲节 之际 , 应 采儿 发布 了 一张 她 和 陈小春 怀抱 他人 婴儿 的 照片 , 并 公开 喊话 陈小春 , 要求 明年 过 上 父亲节 。\n",
      "output:  <UNK> 预测 : 或 <UNK> <EOS>\n",
      "target:  应 采儿 陈小春 晒 “ 三口 三口 ” ” 照 <EOS>\n",
      "\n",
      " input:  本报 北京 7 月 3 日电 由 日本 国际 协力 机构 主办 的 中日 环境污染 健康 损害赔偿 制度 研讨会 于 7 月 3 日 — 4 日 在 北京 举行 。\n",
      "output:  韩国 各项 论坛 : 开展 <EOS>\n",
      "target:  中日 环境 研讨会 在京举行 <EOS>\n",
      "\n",
      " input:  晨报讯 <UNK> 了 , 老婆 <UNK> 了 , 他 悄悄 下 床 , 到 楼下 <UNK> 道 , 从 门缝 中 偷窥 女子 洗澡 , 被 女子 男友 当场 抓获 。\n",
      "output:  女子 被 <UNK> 曝 买 男子 被 行政拘留 <EOS>\n",
      "target:  老婆 <UNK> 了 他 <UNK> 邻居 洗澡 <EOS>\n",
      "\n",
      " input:  国际 在线 消息 : 据 《 联合早报 》 报道 : 菲律宾 军方 日前 警告 称 , 包括 记者 在内 的 外国人 赴 菲律宾 南部 动荡 地区 可能 遭 恐怖组织 绑架 。\n",
      "output:  菲律宾 军方 警告 外国人 赴 菲 南部 地区 可能 遭绑架 <EOS>\n",
      "target:  菲 军方 警告 外国人 赴 菲 南部 地区 可能 遭绑架 <EOS>\n",
      "\n",
      " input:  据 中国 地震 台网 消息 , 2012 年 07 月 20 日 20 时 11 分在 江苏省 扬州市 高邮市 、 宝应县 交界 发生 4.9 级 地震 , 震源 深度 5 公里 。\n",
      "output:  江苏 丽江 解决 地震 市 期货 要求 美元 21 亿元 <EOS>\n",
      "target:  江苏 扬州 发 4.9 级 地震 系 一个月 内 第二次 <EOS>\n",
      "\n",
      " input:  近日 , 海外 媒体 曝光 了 路 虎 揽胜 极光 高性能 版 R 的 效果图 , 根据 之前 路虎 就 有 计划 推 高性能 版 计划 的 报道 , 这 款 高性能 版 R 将 有望 实现 研发 推出 。\n",
      "output:  三个 再现 卫星 照片 有望 落户 足球 <EOS>\n",
      "target:  揽胜 极光 高性能 版 R 曝光 搭载 V6 引擎 <EOS>\n",
      "\n",
      " input:  随着 气温 逐渐 升高 , 加强 夏季 苗木 管护 工作 <UNK> 。 为此 , 山东省 邹城市 林业局 突出重点 狠抓 “ 三夏 ” 苗木 管护 工作 , 积极 采取 相关 措施 , 加强 保护 管理 。\n",
      "output:  新疆 铁通 加强 优秀 代表 <UNK> “ 七一 ” <EOS>\n",
      "target:  山东省 <UNK> 林业局 突出重点 狠抓 “ 三夏 ” 苗木 <UNK> <EOS>\n",
      "\n",
      " input:  中新网 6 月 25 日电 据 外媒 报道 , 法院 判处 父母 对 子女 监护权 无效 的 理由 可以 有 多种 , 可 近日 , 加拿大 一名 男子 却 因 身体 太胖 从而 被 法院 剥夺 该项 权利 。\n",
      "output:  男子 重 重 男子 因 神秘 <UNK> 重 鳄鱼 可 上网 发 “ 8\n",
      "target:  男子 重 172 公斤 法院 <UNK> 胖 剥夺 对 孩子 监护权 <EOS>\n",
      "\n",
      " input:  中新网 宁德 7 月 4 日电 厦门大学 南洋 研究院 暨 国际关系学院 <UNK> 兵 老师 近日 带领 “ 青年 集合 ” 社会 实践 <UNK> 有着 “ 闽东 第一 侨乡 ” 之称 的 福建 宁德市 古田县 大桥 镇 做 实地 调研 。\n",
      "output:  “ 泰国 台湾 泰国 台湾 台湾 台湾 ” 台湾 主题 农业 训练 <EOS>\n",
      "target:  厦大 社会 实践 队 走进 闽东 第一 侨乡 调研 华侨 与 <UNK> <EOS>\n",
      "\n",
      " input:  本报 周口 讯 记者 昨日 从 周口市 教育局 获悉 , 今年 高考 , 郸城 <UNK> 被 清华 、 北大 两校 共 录取 23 人 , 在 两所 大学 录取人数 上 该校 位居 全省 第一 。\n",
      "output:  成都 拟 学生 考入 <UNK> 手术 <EOS>\n",
      "target:  周口 一 县级 高中 郸城 一高 23 名 学生 考入 北大清华 <EOS>\n",
      "\n",
      " input:  19 日 上午 , 天桥区 首届 高校 龙舟 对抗赛 在 西 工商 河 举行 。 来自 山东 科技 大学 、 山东 交通 学院 的 两支 龙舟队 经过 <UNK> 制 比赛 决出 优胜者 。\n",
      "output:  青岛 国际 在 南沙 啤酒节 <UNK> 开幕 <EOS>\n",
      "target:  工商 河里 赛龙舟 <EOS>\n",
      "\n",
      " input:  本报 北京 6 月 16 日电 国务院 机关 事务 管理局 和 教育部 今天 在京举行 “ 我 与 节水 同行 ” 创建 <UNK> 校园 主题 宣传 活动 , 启动 <UNK> 校园 建设 工作 。\n",
      "output:  区 领导 要 帮扶 发展 <EOS>\n",
      "target:  我国 积极 推进 节约型 校园 建设 <EOS>\n",
      "\n",
      " input:  中新网 7 月 22 日电 据 中央电视台 报道 , 美国 科罗拉多州 枪击案 最新进展 : 当地 时间 21 日 上午 11 点 40 分 左右 , 警方 成功 拆除 了 枪击案 犯罪 嫌疑人 霍尔姆 斯 家里 的 首个 爆炸 装置 。\n",
      "output:  美国 科州 枪击案 : 嫌犯 家中 首个 爆炸 装置 被 拆除 <EOS>\n",
      "target:  美国 科州 枪击案 : 嫌犯 家中 首个 爆炸 装置 被 拆除 <EOS>\n",
      "\n",
      " input:  中新网 鲁山 6 月 21 日电 近日 , 就职 于 美国 阿拉 巴玛 大学 的 华侨 郭军鹏 教授 到 河南 鲁山 一 高新 校区 参观 , 看到 母校 巨大 的 变化 后 , 他 十分 的 骄傲 和 自豪 。\n",
      "output:  美国 阿拉 巴玛 大学 华人 教授 回乡 探 母校 感慨 巨变 <EOS>\n",
      "target:  美国 阿拉 巴玛 大学 华人 教授 回乡 探 母校 感慨 巨变 <EOS>\n",
      "\n",
      " input:  本报 7 月 12 日讯 12 日 , 记者 从 德州 出入境 检验 检疫局 获悉 , 今年 上半年 , 德州 出入境 检验 检疫局 帮助 出口 企业 减免 进口国 关税 6000 万元 人民币 。\n",
      "output:  四川 企业 “ <UNK> ” 成绩 出炉 <EOS>\n",
      "target:  出口 企业 减免 <UNK> <UNK> <EOS>\n",
      "\n",
      " input:  中新社 北京 6 月 21 日电 中国政协 第十一届 全国 委员会 常务委员会 第十八次 会议 21 日 通过 “ 关于 接受 梁振英 请求 辞去 中国人民政治协商会议 第十一届 全国 委员会 常务委员 、 委员 职务 的 决定 ” 。\n",
      "output:  梁振英 请辞 全国政协 常委 委员 职务 获准 <EOS>\n",
      "target:  梁振英 请辞 全国政协 常委 委员 职务 获准 <EOS>\n",
      "\n",
      " input:  张女士 问 : 北京 这 几天 雨水 较 多 , 我 儿子 在 上学 的 路上 被 折断 的 枯树枝 砸伤 了 , 孩子 还 在 治疗 , 请问 我 该 找 谁 承担责任 ?\n",
      "output:  欧洲杯 : 不 欧洲杯 玩 意外 <UNK> 再生 不幸 哭 女孩 <UNK> 万元 <EOS>\n",
      "target:  树木 砸伤 人 权属 部门 应 担责 <EOS>\n",
      "\n",
      " input:  7 月 9 日 22 时 25 分 , 接 汉中市 防办 报告 , 南郑县 法镇 及 红庙 镇 先后 因 山体 滑坡 和 泥石流 灾害 造成 4 人 失踪 , 1 人 受伤 。\n",
      "output:  陕西 汉中 山体 滑坡 和 泥石流 灾害 致 4 人 失踪 <EOS>\n",
      "target:  陕西 南郑县 发生 山体 滑坡 和 泥石流 4 人 失踪 1 人伤 <EOS>\n",
      "\n",
      "\n",
      "** bucket 2\n",
      "test examples: 1240\n",
      "loss =   9.8734\n",
      " input:  7 月 1 日 , 由 西安 开往 延安 的 D5090 次列车 在 西安 北站 等候 发车 。 当日 , 西安 至 延安 动车组 正式 开通 运营 , 列车 经由 <UNK> 高铁 、 包 西线 运行 , 每天 一对 , 全程 2.5 小时 。 新华社 记者 李一博 摄\n",
      "output:  西安 看 “ 小 的 ” <EOS>\n",
      "target:  西安 至 延安 动车组 正式 开行 <EOS>\n",
      "\n",
      " input:  本报讯 记者 姚芃 中国消费者协会 近日 发布 警示 称 , 公路 晒 粮 违反 公路 法 、 道路交通 安全法 等 法律法规 , 给 交通安全 造成 隐患 , 呼吁 广大 农民 自觉 守法 , 同时 建议 尽可能 设置 统一 粮食 <UNK> 场地 。\n",
      "output:  建 举报 <UNK> 帮扶 六成 <EOS>\n",
      "target:  公路 晒 粮 危及 交通 污染 粮食 <EOS>\n",
      "\n",
      " input:  据 新华社 杭州 6 月 16 日电 浙江 省农业厅 近日 在 全省 杨梅 主产区 进行 质量 安全 专项 督查 , 对 宁波 、 台州 、 温州 、 丽水 、 金华 等 主产区 38 个 批次 样品 进行 质量 抽检 , 检测 结果 全部 合格 。\n",
      "output:  广东 高考 犯罪 “ 6 <UNK> ” 6 个 文物 前 4 个 <EOS>\n",
      "target:  浙江 杨梅 质量 抽检 结果 全部 合格 <EOS>\n",
      "\n",
      " input:  国际 在线 消息 : 据 新华社 电 , 《 宫廷 的 诱惑 中国 饮食文化 展 》 18 日 在 位于 巴黎埃菲尔铁塔 旁 的 盖 <UNK> 博物馆 开幕 。 展览 利用 各种 烹饪 和 饮食 器皿 , 向 观众 讲述 7000 年来 中国 饮食文化 的 发展 变迁 和 地域 差异 。\n",
      "output:  中国 国际 近 4 国际 “ 中国 好 中国 移民 ” 向 中国 无关\n",
      "target:  中国 饮食文化 展在 巴黎 开展 讲述 7000 年 饮食文化 <EOS>\n",
      "\n",
      " input:  环球 外汇 7 月 10 日讯 欧洲央行 管委会 成员 诺亚 周二 表示 , 欧洲央行 准备 在 其 职能 框架 内 就 欧债 危机 采取 进一步 行动 。 此外 , 他 还 并 敦促 各国 政府 迅速 推进 危机 应对 机制 。\n",
      "output:  <UNK> : 德法 进一步 最大 领导人 并 制定 并 应对 基金 <EOS>\n",
      "target:  欧洲央行 诺亚 : 欧洲央行 将 进一步 采取措施 应对 危机 <EOS>\n",
      "\n",
      " input:  本届 欧洲杯 开幕 以来 , 各路 动物界 的 <UNK> 纷纷 亮相 , 对 大赛 进行 预测 。 令 英格兰 人 感到 振奋 的 是 , 他们 的 神兽 “ 尼古拉斯 ” 终于 出手 了 , 而且 预测 最后 的 冠军 就是 英格兰 !\n",
      "output:  英 <UNK> <UNK> <UNK> <UNK> 状态 <UNK> <EOS>\n",
      "target:  终极 <UNK> 羊驼 出手 <EOS>\n",
      "\n",
      " input:  李长春 河南 调研 <UNK> 这 是 他 到 中央 后 第四次 回来 他 说 每次 回来 都 有 新 收获 新 感动 , 这次 尤其 如此 充分肯定 新型农村 社区 建设 : 能 让 贫困地区 发生 <UNK> , 平原 地区 就 更 有 希望\n",
      "output:  英 <UNK> : 东莞 奥运 出新 或 就 出版 <EOS>\n",
      "target:  四 回 河南 <UNK> <EOS>\n",
      "\n",
      " input:  保险 板块 6 月 18 日 盘中 走势 分化 , 截至 10 : 11 , 新华 保险 下跌 1.91 % , 中国 人寿 下跌 0.55 % 。 此外 , 中国 太保 上涨 0.54 % , 中国 平安 上涨 0.15 % 。\n",
      "output:  食品 板块 表现 走低 包钢 稀土 跌 2.7 % 环保 股 <EOS>\n",
      "target:  快讯 : 保险 板块 走势 分化 新华 保险 跌近 2 % <EOS>\n",
      "\n",
      " input:  上海市 宣传 系统 纪念 建党 91 周年 暨 创先争优 活动 表彰大会 日前 在 上海图书馆 召开 , 大会 对 宣传 系统 20 个 先进 基层 党组织 和 30 名 优秀 共产党员 进行 了 表彰 。 市委常委 、 宣传部 部长 杨振武 出席会议 并 讲话 。\n",
      "output:  宣传 工作 在 京 启动 <EOS>\n",
      "target:  上海市 宣传 系统 举行 创先争优 表彰大会 <EOS>\n",
      "\n",
      " input:  6 处 路桥 涵洞 排水 改造 完成 本报讯 昨日 , 市政协 委员 一行 来到 市 水务局 , 对 关于 全市 <UNK> 系统 规划 建设 的 重点 提案 进行 了 督办 。 据 了解 , 竹叶山 等 6 处 路桥 涵洞 的 排水 改造 已 基本 完成 。\n",
      "output:  <UNK> 集团 推进 企业 建 <UNK> 5 万吨 刺激 <EOS>\n",
      "target:  6 处 路桥 涵洞 排水 改造 完成 <EOS>\n",
      "\n",
      " input:  近日 , 台山 核电 一期 <UNK> 安装 工程 1 号 机组 压力容器 完成 <UNK> 。 本次 测量 采用 <UNK> <UNK> 核电 建设 特有 的 “ <UNK> 主 系统 三维 测量 ” 技术 , 是 <UNK> 核电 3 大 关键技术 之一 , 在 国内 尚属 首次 实际 应用 。\n",
      "output:  <UNK> <UNK> 牧民 落成 <EOS>\n",
      "target:  台山 核电 1 号 机组 <UNK> 完成 安装 测量 <EOS>\n",
      "\n",
      " input:  华奥 星空 讯 据 北戴河 体育局 消息 , 在 2012 年 7 月 7 日 举办 的 中国 体育彩票 石家庄 电视塔 轮滑 大赛 上 , 北戴河 轮滑 队以 9 金 4 银 1 铜 的 成绩 获 金牌 总数 和 团体 总分 双 第一 的 好 战绩 。\n",
      "output:  中国 首部 首部 比赛 在 新加坡 10 月 上 比赛 与 比赛 举办 <EOS>\n",
      "target:  石家庄 电视塔 轮滑 大赛 日前 落幕 北戴河 队 团体 夺冠 <EOS>\n",
      "\n",
      " input:  谁 来 挽救 低迷 中 的 法国 足球 ? 曾经 给 法国 足球 带来 无数 荣誉 的 齐祖站 了 出来 。 昨天 , 法国 足协 主席 勒 格拉 埃 透露 前 世界足球 先生 齐达内 有意 出 任高卢 雄鸡 的 主帅 。\n",
      "output:  齐达内 有意 执教 法国队 <EOS>\n",
      "target:  齐达内 进军 教练 <UNK> 有 准备 去年 <UNK> 得 教练 证书 <EOS>\n",
      "\n",
      " input:  昨天 , 中粮 地产 发布公告 称 , 全资 子公司 中粮 地产 集团 深圳 房地产 开发 有限公司 与 深圳市 前 海 深港 现代 生活 服务业 合作 区 管理局 签订 合作 框架 协议 。 受此 影响 , 中粮 地产 强势 涨停 , 报收 4.42 元 。\n",
      "output:  两 公司 投资 获 <UNK> 研发 <EOS>\n",
      "target:  进军 深圳 前 海 中粮 地产 强势 涨停 <EOS>\n",
      "\n",
      " input:  23 日 慕尼黑 市中心 广场 上 1700 名 裸体 志愿者 全身 被 喷红 漆作 艺术 模特 。 艺术 模特 根据 摄影师 的 指令 作出 统一 的 动作 。 摄影师 现场 指挥 裸体 模特 23 日 慕尼黑 市中心 广场 上 1700 名 裸体 志愿者 全身 被 喷红 漆作 艺术 模特 。\n",
      "output:  德国 千余 裸体 人士 喷红漆 进行 艺术表演 <EOS>\n",
      "target:  德国 千余 裸体 人士 喷红漆 进行 艺术表演 <EOS>\n",
      "\n",
      " input:  发改委 称 饮水 安全 将 纳入 地方 考核 昨天 , 国家 发展 和 改革 委员会 副 主任 杜鹰 在 第十一届 全国人大常委会 第二十七次 会议 上作 关于 保障 饮用水 安全 工作 情况 的 报告 时说 , 中国 城市 供水 水质 不 达标 问题 突出 。\n",
      "output:  中国 加大 鼓励 改革 “ <UNK> ” 在 墨西哥 四大 工作 <EOS>\n",
      "target:  发改委 : 城市 供水 水质 不 达标 突出 <EOS>\n",
      "\n",
      " input:  人民网 海南 视窗 海口 7 月 17 日电 7 月 17 日 上午 , 海南省 第四届 人民代表大会常务委员会 第三十二 次 会议 通过 了 《 海南省 人大常委会 关于 成立 三 沙市 人民代表大会 筹备组 的 决定 》 , 这 标志 着 三 沙市 的 政权 组建 正式 启动 。\n",
      "output:  三 沙市 人民代表大会 筹备组 成立 <EOS>\n",
      "target:  三 沙市 政权 组建 正式 启动 将 选举 产生 政府 机构 <EOS>\n",
      "\n",
      " input:  东方网 6 月 24 日 消息 : 据 《 东方 早报 》 报道 , 印象 中 的 缉毒 犬 是不是 <UNK> <UNK> ? 其实 它们 <UNK> , 平时 也 <UNK> ! 今天 , 海关 将 在 上海海关 业务 二处 <UNK> 物品 监管 中心 开展 缉毒 犬 开放日 活动 。\n",
      "output:  《 <UNK> 青少年 和 <UNK> 》 : 浙江 <UNK> 拍 <EOS>\n",
      "target:  今天 海关 邀 市民 与 缉毒 犬 亲密 互动 <EOS>\n",
      "\n",
      " input:  商报 济南 消息 昨天 , 记者 从省 招考 院 获悉 , 艺术 、 体育类 本科 一批 第二次 征集 志愿 开始 投档 、 录检 , 共 <UNK> 考生 <UNK> 人 , 尚有 近百 所 院校 “ <UNK> ” , 不满 院校 将 通过 调剂 来 补充 生源 。\n",
      "output:  “ 农村 直通车 ” <EOS>\n",
      "target:  <UNK> 类 一本 二次 征集 仍 “ <UNK> ” <EOS>\n",
      "\n",
      " input:  本报讯 昨日 上午 , 雁塔区 在 曲江 <UNK> 世家 项目 工地 举行 2012 年 第二批 重点 建设项目 集中 开工 仪式 , 20 个 重点 建设项目 集中 开工 。 市委常委 、 常务副 市长 岳华峰 , 市 人大常委会 副 主任 <UNK> , 市政协 副 主席 <UNK> 出席 开工 仪式 。\n",
      "output:  市 项目 开工 建设 总 开工 建设 <EOS>\n",
      "target:  20 个 重点项目 总 投资 <UNK> 亿 <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_examples_to_print = 20\n",
    "\n",
    "def cut_at_eos(sentence):\n",
    "    if _EOS in sentence:        \n",
    "        return sentence[:sentence.index(_EOS)+1]\n",
    "    else:\n",
    "        return sentence\n",
    "\n",
    "def no_prepending_pad(sentence):\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i] != _PAD:\n",
    "            return sentence[i:]\n",
    "\n",
    "with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=True):\n",
    "    outputs, losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n",
    "        encoder_placeholders, decoder_placeholders, target_placeholders,\n",
    "        target_weights_placeholders, buckets, lambda x, y: seq2seq_f(x, y, True),\n",
    "        softmax_loss_function=sampled_loss)\n",
    "    \n",
    "    if output_projection is not None:\n",
    "        for bucket_id in range(len(buckets)):\n",
    "            outputs[bucket_id] = [\n",
    "                tf.matmul(output, output_projection[0]) + output_projection[1]\n",
    "                for output in outputs[bucket_id]]\n",
    "\n",
    "    for bucket_id in range(len(buckets)):\n",
    "        print('\\n** bucket {}'.format(bucket_id))\n",
    "        cur_data_size = data_sizes_test[bucket_id]\n",
    "        print('test examples: {}'.format(cur_data_size))\n",
    "        if cur_data_size == 0:\n",
    "            continue  # 某个 bucket 为空的特殊情形\n",
    "\n",
    "        encoder_size, decoder_size = buckets[bucket_id]\n",
    "        bucket_inputs_test = get_bucket_inputs(encoder_data_test, decoder_data_test, bucket_id)\n",
    "        bucket_feed = generate_feed_dict(bucket_inputs_test, encoder_size, decoder_size)\n",
    "\n",
    "        loss_val = sess.run(losses[bucket_id], feed_dict=bucket_feed)\n",
    "        print('loss = {: 8.4f}'.format(loss_val))\n",
    "        \n",
    "        output_bucket = np.zeros((cur_data_size, decoder_size), dtype=int)  \n",
    "        # output_bucket 用于记录当前bucket输出值, 形状是 outputs 的\"转置\"\n",
    "    \n",
    "        for i in range(decoder_size):\n",
    "            prob = outputs[bucket_id][i]  # 第i个词的概率输出\n",
    "            output_bucket[:, i] = np.argmax(sess.run(prob, feed_dict=bucket_feed), axis=1)\n",
    "        \n",
    "        # for j in range(cur_data_size):\n",
    "        for j in range(num_examples_to_print):  # 只看 bucket 里的前几个句子\n",
    "            sen = [vocab_dec[output_bucket[j, k]] for k in range(decoder_size)]\n",
    "            input_ = [vocab_enc[i] for i in encoder_data_test[bucket_id][j]]\n",
    "            target_ = [vocab_dec[i] for i in decoder_data_test[bucket_id][j][1:]]\n",
    "            input_ = no_prepending_pad(input_)\n",
    "            sen = cut_at_eos(sen)\n",
    "            target_ = cut_at_eos(target_)\n",
    "            print(' input: ', ' '.join(input_))\n",
    "            print('output: ', ' '.join(sen))\n",
    "            print('target: ', ' '.join(target_))\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "1351px",
    "left": "0px",
    "right": "1038px",
    "top": "106.989px",
    "width": "135px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
